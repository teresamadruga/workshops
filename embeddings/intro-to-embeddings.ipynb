{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b98e65c-0789-432b-8e53-b7b89bb31d9e",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "Embeddings are numerical representations of objects, such as words, sentences, or images, in a high-dimensional space. The purpose of embeddings is to capture and encode the underlying semantics and relationships between objects in a way that is suitable for computational analysis.\n",
    "\n",
    "In the context of NLP, word embeddings are commonly used. They represent words as dense, continuous vectors in a high-dimensional space, where each dimension corresponds to a particular feature or aspect of the word. The key idea behind word embeddings is that similar words are represented by similar vectors that are close to each other in the embedding space.\n",
    "\n",
    "Word embeddings are typically learned from large amounts of text data using unsupervised learning techniques, such as word2vec or GloVe. These algorithms consider the distributional properties of words, meaning that they look at the co-occurrence patterns of words in sentences. The basic assumption is that words appearing in similar contexts are likely to have similar meanings.\n",
    "\n",
    "During the training process, the algorithm constructs a high-dimensional vector space where words are positioned. Initially, the word vectors are randomly initialized, and then the model is trained to predict the probability of a word appearing in the context of other words. This prediction task forces the model to learn meaningful representations, as similar words should have similar context distributions.\n",
    "\n",
    "Once the training is complete, the learned word embeddings can be used for a variety of NLP tasks. For example, in text classification, embeddings can be used as input features for a machine learning model, allowing it to capture semantic similarities between words and make better predictions. Similarly, in information retrieval, embeddings can be used to measure the similarity between query and document vectors, enabling efficient and accurate document ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fca6f8-8a71-4d39-a8ea-f4b13c096b11",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is a task in Natural Language Processing (NLP) that involves breaking down a text into smaller linguistic units called tokens. Tokens can be words, subwords, characters, or even smaller units, depending on the tokenization method used. Here are some different types of tokenization commonly used in NLP:\n",
    "\n",
    "- Word Tokenization: Word tokenization is the most basic and commonly used form of tokenization. It involves splitting text into individual words based on whitespace or punctuation marks. For example, the sentence \"I love NLP!\" would be tokenized into [\"I\", \"love\", \"NLP\", \"!\"].\n",
    "\n",
    "- Sentence Tokenization: Sentence tokenization involves splitting a text into individual sentences. This is useful for tasks that require processing text on a sentence level, such as machine translation or sentiment analysis. For example, the paragraph \"I love NLP! It's fascinating. Don't you think?\" would be tokenized into [\"I love NLP!\", \"It's fascinating.\", \"Don't you think?\"].\n",
    "\n",
    "- Subword Tokenization: Subword tokenization is useful for languages with complex word structures or when dealing with out-of-vocabulary (OOV) words. It splits words into smaller subword units, allowing the model to handle unseen words by leveraging subword units it has encountered during training. Common subword tokenization algorithms include Byte-Pair Encoding (BPE) and Unigram Language Model.\n",
    "\n",
    "- Character Tokenization: Character tokenization treats each character in a text as a separate token. This level of tokenization is useful when dealing with morphologically rich languages, where words can have various inflections and affixes. It also enables character-level modeling and handling of out-of-vocabulary words.\n",
    "\n",
    "- Custom Tokenization: In certain cases, custom tokenization techniques may be required based on specific requirements or domain-specific tasks. This involves designing tokenization rules and patterns tailored to the specific needs of the task at hand. For example, you might want to tokenize product names or social media hashtags in a specific way to capture their significance.\n",
    "\n",
    "The choice of tokenization method depends on the specific task, language, and data at hand. Different tokenization approaches have their advantages and disadvantages, and selecting the appropriate method is crucial for downstream NLP tasks and model performance.\n",
    "\n",
    "Tokenization and embeddings are closely related in NLP, as tokenization plays a crucial role in determining the input units for which embeddings are generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98382c9-529e-4e1a-bb11-97a110f9f9ee",
   "metadata": {},
   "source": [
    "# Word2Vec - [Paper Link](https://arxiv.org/abs/1301.3781)\n",
    "The word2vec paper, titled \"Efficient Estimation of Word Representations in Vector Space,\" was published by Tomas Mikolov et al. in 2013. The paper introduced two models, Continuous Bag-of-Words (CBOW) and Skip-Gram, for learning word embeddings from large text corpora efficiently. These word embeddings capture semantic relationships between words and have become widely used in natural language processing tasks.\n",
    "\n",
    "The paper starts by discussing the shortcomings of traditional approaches to representing words as one-hot encoded vectors, which are high-dimensional and sparse. It introduces the idea of distributed representations, where each word is represented by a dense vector in a continuous vector space. These vectors capture semantic similarities and can be used to perform various NLP tasks.\n",
    "\n",
    "Continuous Bag-of-Words (CBOW): The CBOW model aims to predict a target word given its context (surrounding words). It takes a fixed-size window of context words and learns to predict the target word at the center of the window. The input context words are represented as one-hot encoded vectors, and a hidden layer is introduced to encode the context information. The model is trained using a neural network with a softmax output layer to predict the target word. The objective is to maximize the average log probability of the target words across the entire corpus.\n",
    "\n",
    "Skip-Gram: The Skip-Gram model is the inverse of CBOW. Instead of predicting the target word from the context, Skip-Gram aims to predict the context words given a target word. The input is a target word, and the output is a set of context words within a fixed window size. The model is trained using a similar neural network architecture as CBOW but with a different objective. It maximizes the average log probability of the context words given the target words.\n",
    "\n",
    "The paper describes the training process using a technique called negative sampling. Instead of using the traditional softmax function that involves computing probabilities for all words in the vocabulary, negative sampling randomly samples a small number of negative (non-context) words to be compared against the true context words. This simplifies the training process and makes it computationally efficient.\n",
    "\n",
    "The authors evaluate the word2vec models on various tasks, including word analogy, word similarity, and word clustering. They show that the learned word embeddings capture semantic relationships, such as analogies like \"king:queen\" or \"man:woman,\" and demonstrate state-of-the-art performance on tasks like word similarity and clustering.\n",
    "\n",
    "The word2vec paper made significant contributions to the field of NLP by introducing efficient methods to learn word embeddings from large corpora. It sparked a renewed interest in distributed representations and paved the way for subsequent advancements in word embeddings, such as GloVe and contextual embeddings like BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125eced4-5ecd-4eac-8b97-fbaf0dbd916a",
   "metadata": {},
   "source": [
    "# Contextual embeddings\n",
    "Contextual embeddings are a type of word representation in natural language processing (NLP) that capture contextual information and meaning by considering the surrounding words in a sentence. Unlike traditional word embeddings that assign a fixed vector representation to each word regardless of its context, contextual embeddings generate dynamic representations that vary based on the context in which the word appears.\n",
    "\n",
    "One of the most popular models for generating contextual embeddings is BERT (Bidirectional Encoder Representations from Transformers). BERT utilizes a deep transformer architecture to learn contextual representations of words. During the pre-training phase, BERT is trained on large amounts of unlabeled text, where it learns to predict missing words within sentences. This pre-training process enables BERT to capture the relationships between words and understand the semantics and syntactic structures of the language.\n",
    "\n",
    "The key idea behind contextual embeddings is that the meaning of a word can vary based on its context. For example, in the sentence \"I saw a bear in the park,\" the word \"bear\" can refer to an animal. However, in the sentence \"I bear no responsibility for this,\" the word \"bear\" has a different meaning, indicating carrying or enduring something. Contextual embeddings aim to capture these nuances and provide representations that encode both the word's inherent meaning and its contextual variations.\n",
    "\n",
    "Contextual embeddings have shown significant improvements in a wide range of NLP tasks, such as text classification, named entity recognition, sentiment analysis, question answering, and machine translation. By considering the surrounding words, contextual embeddings can better handle polysemy (words with multiple meanings) and resolve syntactic ambiguities in a sentence.\n",
    "\n",
    "Contextual embeddings are obtained by applying a pre-trained model, such as BERT, to a specific task. During the fine-tuning stage, the pre-trained model is adapted to the specific downstream task by adding task-specific layers and training on labeled data. The contextual embeddings from the pre-trained model are often used as input features for downstream models, allowing them to leverage the learned contextual information.\n",
    "\n",
    "Other models, such as GPT (Generative Pre-trained Transformer) and XLNet, also generate contextual embeddings. These models have variations in their architecture and training objectives but share the common goal of capturing context-dependent representations. Each model has its own strengths and characteristics, and the choice of contextual embedding model depends on the specific task and data requirements.\n",
    "\n",
    "Overall, contextual embeddings have greatly improved NLP by capturing the context-dependent nature of language. They provide a more nuanced and accurate representation of words, enabling models to better understand and generate human-like text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55cc7f-4da1-4ecf-ab1f-89547014192b",
   "metadata": {},
   "source": [
    "# Contextual embeddings in GPT 3+\n",
    "GPT-3 (Generative Pre-trained Transformer 3) is a state-of-the-art language model that utilizes a deep transformer architecture, similar to its predecessors GPT-1 and GPT-2. The core idea behind GPT-3 is unsupervised pre-training on a large corpus of text data followed by fine-tuning on specific downstream tasks.\n",
    "\n",
    "GPT-3 generates contextual embeddings by leveraging the transformer architecture, which captures the relationships between words based on their positions in the input sequence. Each token in the input, such as a word or subword, is associated with a learned embedding vector. These embeddings are contextual because they consider the entire input sequence and the interactions between tokens during the training process.\n",
    "\n",
    "The transformer architecture of GPT-3 consists of self-attention mechanisms, which allow the model to capture dependencies and relationships between all tokens in the input sequence. The attention mechanism helps the model focus on relevant parts of the input when generating embeddings and making predictions.\n",
    "\n",
    "During the pre-training phase, GPT-3 is trained on a large corpus of text data, such as books, articles, and the internet, using a language modeling objective. This objective involves predicting the next word in a sentence given the preceding words. This process enables the model to learn the distributional properties of the language and develop a rich understanding of contextual information.\n",
    "\n",
    "Once pre-training is complete, GPT-3 can be fine-tuned on specific downstream tasks by providing task-specific training data and adjusting the model's parameters. This fine-tuning process adapts the pre-trained contextual embeddings to the target task, enhancing the model's performance and enabling it to generate contextually appropriate responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b4cc7f-0ecf-4797-948e-b8527b2553cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
